from selenium import webdriver
from pyvirtualdisplay import Display
import re
import hashlib
import socket
import json
import requests

import db_utils
import utils
import time

from pprint import pprint

USER_AGENT_STRING = "Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.76 Safari/537.36"

capabilities = webdriver.DesiredCapabilities.CHROME
capabilities["chrome.switches"] = ["--user-agent="+USER_AGENT_STRING] 


WHITELISTED_DOMAINS = [
    'cloudfront',
    'google',
    'aws',
    'facebook',
    'twitter',
    'aws',
    'amazon',
    'jquery',
    'packtpub',
    'statcounter',
    'wordpress',
    'static',
    'rackcdn',
    'asset',
    'doubleclick',
    'cnet',
    'cdn',
    'msn',
    'yahoo',
    'aol',
    'newsgator',
    'rojo',
    'feedster',
    'mixpanel',
    'youtube',
    'imageshack',
    'dmca',
    'informer',
    'gravatar',
    'linkedin',
    'piwik',
    'bestessay',
    'w3.org',
    'wpcustomer',
    'piwik',
    'json',
    'instagram',
    'xkcd',
    'sitemeter',
    'tribalfusion',
    'quantserve',
    'addthis',
    'marketgid',
    'intellitext',
    'sharethis',
    'carbonads',
    'torrent',
    'appegs',
    'gowebsolutions',
    'alexking',
    'mailchimp',
    'del.icio.us',
    'digg',
    'vibrantmedia',
    'stats',
]


class WebPage(object):
    def __init__(self, url, status_code=200):
        self.url = utils.sanitize_url(url)
        print"Creating Webpage object for url: ", self.url
        self.url_hash = self._url_hash()
        self.status_code = 200 #self._get_status_code()
        self.domain = self._domain()
        self.host = "0.0.0.0" #self._host()
        self.geo_location = "None"#utils.getGeoLocation(str(self.host))
        self.incoming_links = set([])
        self.a_links = set([])
        self.js_links = set([])
    
    def _get_status_code(self):
        return utils.get_status_code(self.url)

    def _url_hash(self):
        return utils.get_url_hash(self.url)

    def _domain(self):
        return utils.domainOf(self.url)
    
    def _host(self):
        if self.status_code==200:
            return socket.gethostbyname(self._domain())
    
    def json(self):
        webpage = {
            'url': self.url,
            'url_hash': self.url_hash,
            'status_code': self.status_code,
            'domain': self.domain,
            'host': self.host,
            'geoLocation': self.geo_location,
            'incoming_links': list(self.incoming_links),
            'a_links': list(self.a_links),
            'js_links': list(self.js_links)
        }
        
        #pprint(webpage)
        return webpage

def isWhiteListedURL(url):
    for u in WHITELISTED_DOMAINS:
        if u in url:
            return True
    return False


def prepareURLsForNextCrawl(urls):
    new_urls = []
    for url in urls:
        url = utils.sanitize_url(url)
        if url.endswith(".js") or url.endswith("exe"):
            url = utils.domainOf(url)
            url = utils.sanitize_url(url)
        new_urls.append(url)
    new_urls = [url for url in new_urls if not isWhiteListedURL(url)]
    return list(set(new_urls))

def persistURLsForNextCrawlInDB(db, urls):
    urls = prepareURLsForNextCrawl(urls)
    crawl_queue = db.crawl_queue
    visited_urls = db.visited_urls

    for url in urls:
        domain = utils.domainOf(url)
        domain_hash = utils.get_url_hash(domain)
        url_hash = utils.get_url_hash(url)

        db_query = {domain_hash: domain}
        update = {url_hash: url}
        url_obj = visited_urls.find_one(db_query)
        if not url_obj:
            crawl_queue.insert(update)


def regex_domain_match(db, url):
    visited_domain_patterns = db.domain_regex_patterns.find_one({'domain_regexes': 'regex'})
    regexes = []
    if visited_domain_patterns:
        regexes = visited_domain_patterns["regexes"]
    for regex in regexes:
        if re.search(regex, url, re.I|re.M):
            return True
    domain = utils.domainOf(url)
    if domain.startswith("www."):
        domain = domain.replace("www.", "")
    
    if domain.count('.') > 1:
        spli = domain.split(".")
        domain = domain[-2] + "." + domain[-1]

    regexes.append(domain)
    if not visited_domain_patterns:
        visited_domain_patterns = {
            'domain_regexes': "regex"}

    visited_domain_patterns["regexes"] = list(set(regexes))
    if "_id" in visited_domain_patterns:
        visited_domain_patterns.pop("_id")
    query = {'domain_regexes': 'regex'}
    db.domain_regex_patterns.find_and_modify(query=query,
            update=visited_domain_patterns, upsert=True, new=True)
    return False


def crawl_url(url, headless=True, save_into_db=True):
    print "Crawling URL",url
    iurl_hash = utils.get_url_hash(url)
    update = {iurl_hash: url}

    db = db_utils.getDBInstance()  
    if regex_domain_match(db, url):
        print "Skipping: ",url
        
    db.crawl_queue.remove(update)
    url = utils.sanitize_url(url)
    url_hash = utils.get_url_hash(url)
    db_query = {'url_hash': url_hash}

    if headless:
        display = Display(visible=0, size=(800, 600))
        display.start()

    
    obj_in_db = db.webpages.find_one(db_query)
    
    webpage = None
    if not obj_in_db:
        webpage = WebPage(url)


    browser = webdriver.Chrome(desired_capabilities=capabilities)
    browser.set_page_load_timeout(30)
    #browser.implicitly_wait(5)
    try:
        print "Visiting page: ",url
        if not url.startswith("http"):
            raise Exception

        browser.get(url)
        #time.sleep(1)
    except Exception, e:
        print "Error Occured"
        browser.quit()
        print e
        return -1 


    a_links = set(utils.getOutgoingAnchorLinks(browser)).difference(set([url]))
    js_links = set(utils.getOutgoingJSLinks(browser))

    if webpage:
        webpage.a_links = a_links
        webpage.js_links = js_links
    else:
        obj_in_db["a_links"] = list(a_links)
        obj_in_db["js_links"] = list(js_links)

    browser.quit()
    
    if save_into_db:
        update = None
        if webpage:
            update = webpage.json()
        else:
            update = obj_in_db

        db.webpages.find_and_modify(query=db_query, update=update, new=True, upsert=True)
        
        all_urls = a_links.union(js_links)
        all_urls = prepareURLsForNextCrawl(all_urls) 

        for out_url in all_urls:
            url_hash = utils.get_url_hash(out_url)
            obj = db.webpages.find_one({'url_hash':url_hash})
            if obj:
                print "Updating for: ", out_url
                query = {'url_hash': url_hash}
                obj["incoming_links"].append(url)
                obj["incoming_links"] = list(set(obj["incoming_links"]))
                if "_id" in obj:
                    obj.pop("_id")
                new_obj = db.webpages.find_and_modify(query=query, update=obj,
                        new=True, upsert=True)
            else:
                new_webpage = WebPage(out_url)
                new_webpage.incoming_links.add(url)
                db.webpages.insert(new_webpage.json())
        
        print "Marking  as visited"
        domain = utils.domainOf(url)
        domain_hash = utils.get_url_hash(domain)
        db_query = {domain_hash: domain}
        update = {url_hash: url}
        vis = db.visited_urls.find_and_modify(query=db_query,
                update=db_query, upsert=True, new=True)
        print vis
        print "Updating crawl queue"
        persistURLsForNextCrawlInDB(db, all_urls)
        print "Updated"


if __name__=='__main__':
    #crawl_url("http://sugarudyog.com/index.htm?adasd=asdas&sadgas=afs", headless=True,
    #        save_into_db=True)
    print
    print "Crawling the other url"
    #crawl_url('http://sugarudyog.com/smsreports.htm', headless=True,
    #        save_into_db=True)

    print
    print "Crawling third url"
    #crawl_url('http://sugarudyog.com/softwares.htm', headless=True,
    #        save_into_db=True)

    print
    print "Crawling kingdls url"
    crawl_url('http://kingdls.com/itunes-64?kw=itunes', headless=True,
            save_into_db=True)

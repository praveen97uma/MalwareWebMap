from multiprocessing import Queue, Pool

import crawler
import db_utils
import utils


START_URLS = ["http://59303.dushigezi.com/Fox+Sports.htm",
                "http://www.fengyunmc.com/gmail+log+on+english.htm",
"http://themedownload123.com/",
            "https://www.pcrx.com/lp2.aspx?cfg=339"]


def seedStartURLsInDB(db, start_urls):

    crawl_queue_table = db.crawl_queue

    url_hashes = [{utils.get_url_hash(url): url} for url in start_urls]
    for query in url_hashes:
        crawl_queue_table.find_and_modify(query=query, update=query,
                upsert=True, new=True)

def getUnvisitedURLsFromDB(db, number_of_urls=4):
    crawl_queue_table = db.crawl_queue
    urls = []
    urls_iterator = crawl_queue_table.find()
    for url in urls_iterator:
        values = url.values()
        crawl_uri = ""
        for val in values:
            if isinstance(val, basestring):
                crawl_uri = val
                break
        urls.append(crawl_uri)
        print crawl_uri
        number_of_urls-=1
        if number_of_urls == 0:
            return urls
    return urls



def crawl_manager(number_of_processes=12 ):
    db = db_utils.getDBInstance()
    seedStartURLsInDB(db, START_URLS)
    number_of_iterations = 10 
    while number_of_iterations:
        pool = Pool(number_of_processes)
        urls = getUnvisitedURLsFromDB(db, number_of_processes)
        result = pool.map(crawler.crawl_url, urls)
        
        number_of_iterations -= 1


if __name__ == '__main__':
    crawl_manager()
